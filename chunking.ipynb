{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chunking by paragraph\n",
    "\n",
    "A bit on working with paragraphs / segmenting. Or should that come later?\n",
    "\n",
    "### chunking by percentage\n",
    "\n",
    "It often makes sense to partition your text up for more legible analysis. After all, we frequently want to get a more nuanced sense of how particular modes of analysis might change over the course of a text. To do that, the first necessary action is to divide the text into smaller portions that can be individually analyzed. One of the most common ways of doing this is to partition the text into even units. Below we divide the text of Jacob's Room into 100 even pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'corpus/1922_jacobs_room.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-3ef8b939f5eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'corpus/1922_jacobs_room.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'corpus/1922_jacobs_room.txt'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "filename = 'corpus/1922_jacobs_room.txt'\n",
    "with open(filename, 'r') as fin:\n",
    "    text = fin.read()\n",
    "\n",
    "text_length = len(text)\n",
    "text_chunks = []\n",
    "number_of_chunks = 100\n",
    "for i in range(number_of_chunks):\n",
    "    chunk_size = text_length/number_of_chunks\n",
    "    chunk_start = math.floor(chunk_size * i)\n",
    "    chunk_end = math.floor(chunk_size * (i +1))\n",
    "    text_chunks.append(text[chunk_start:chunk_end])\n",
    "\n",
    "\n",
    "print('number of chunks: ' + str(len(text_chunks)))\n",
    "print('length of chunk 1: ' + str(len(text_chunks[0])))\n",
    "print('length of chunk 2: ' + str(len(text_chunks[1])))\n",
    "print('length of chunk 3: ' + str(len(text_chunks[2])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dividing up the text in this way provides us with a series of small texts, each of which can be subjected to analysis. We can then string the analysis of these smaller pieces to make arguments about trends in the overal piece. Below we take the same bit of code, wrap it into a function, and then use it to track changes in the use of the word Jacob over the course of the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_chunks(text, num_chunks):\n",
    "    text_length = len(text)\n",
    "    text_chunks = []\n",
    "    number_of_chunks = num_chunks\n",
    "    for i in range(number_of_chunks):\n",
    "        chunk_size = text_length/number_of_chunks\n",
    "        chunk_start = math.floor(chunk_size * i)\n",
    "        chunk_end = math.floor(chunk_size * (i +1))\n",
    "        text_chunks.append(text[chunk_start:chunk_end])\n",
    "    return text_chunks\n",
    "\n",
    "filename = 'corpus/1922_jacobs_room.txt'\n",
    "with open(filename, 'r') as fin:\n",
    "    raw_text = fin.read()\n",
    "\n",
    "chunked_text = get_chunks(raw_text, 100)\n",
    "tokenized_text = [nltk.word_tokenize(chunk) for chunk in chunked_text]\n",
    "jacob_counts = [nltk.FreqDist(tokenized_chunk)['Jacob'] for tokenized_chunk in tokenized_text]\n",
    "print(jacob_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember - we have divided the text up into 100 (roughly) equal units. Using the FreqDist() module in the NLTK package we get a quick count of the word 'Jacob' in the text. We can then take that information and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.plot(jacob_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little bit difficult to see any discernable trend lines, right? Fortunately we already have code that can help us parse things a little differently. Rather than slotting things into 100 equal parts, lets shift to ten equal chunks for the whole novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_chunks(text, num_chunks):\n",
    "    text_length = len(text)\n",
    "    text_chunks = []\n",
    "    number_of_chunks = num_chunks\n",
    "    for i in range(number_of_chunks):\n",
    "        chunk_size = text_length/number_of_chunks\n",
    "        chunk_start = math.floor(chunk_size * i)\n",
    "        chunk_end = math.floor(chunk_size * (i +1))\n",
    "        text_chunks.append(text[chunk_start:chunk_end])\n",
    "    return text_chunks\n",
    "\n",
    "filename = 'corpus/1922_jacobs_room.txt'\n",
    "with open(filename, 'r') as fin:\n",
    "    raw_text = fin.read()\n",
    "\n",
    "chunked_text = get_chunks(raw_text, 10)\n",
    "tokenized_text = [nltk.word_tokenize(chunk) for chunk in chunked_text]\n",
    "jacob_counts = [nltk.FreqDist(tokenized_chunk)['Jacob'] for tokenized_chunk in tokenized_text]\n",
    "print(jacob_counts)\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "plt.plot(jacob_counts)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lesson here is that visualizations are constructed and subject to interpretation. The first graph using 100 chunks showed a text with a noisy distribution of results and no clear meaning. The second graph accounted for some of this noise by using a smaller number of chunks, the result being that we can clearly see an increase in the use of Jacob's name over the course of the novel. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
