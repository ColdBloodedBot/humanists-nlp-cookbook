{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Pipeline using a Corpus Class\n",
    "\n",
    "When working with a corpus of texts it can quickly become confusing to keep track of which step in an NLP pipeline you are on. Say you want to run a Frequency Distribution, did you remember to tokenize the text? To pull out the stopwords? While this is simple enough if you are working with a small group of texts in a discrete timeperiod, this quickly becomes challenging when working with a large body of texts or when working over a longer period of time. Matters become more complicated if you want to switch between corpus-level analysis and text-level analysis. The realities of your project may quickly mean that manually performing each step in your pipeline becomes redundant, hard to keep track of, or a waste of time. This is where objects and classes can come in. \n",
    "\n",
    "This can get confusing so we'll start with an example. I own a cat. Cats have certain qualities:\n",
    "\n",
    "* furry\n",
    "* color\n",
    "* four legs\n",
    "* personality\n",
    "\n",
    "And they do certain things:\n",
    "\n",
    "* eat\n",
    "* sleep\n",
    "* scratch\n",
    "* generally enrich the lives of all around them\n",
    "\n",
    "Any one cat might be different than another. Your cat might not have fur. It might have fewer than four legs. It might not enrich your life (hard to believe). What we have here is a set of characteristics and verbs that describe the thing that is a cat. Not all cats, but one type of cat. \n",
    "\n",
    "One more example. Consider a house. We might assume that it has certain qualities: \n",
    "\n",
    "* a roof\n",
    "* a front door\n",
    "* walls\n",
    "* a window\n",
    "\n",
    "And you can do certain things to, with, or in a house:\n",
    "\n",
    "* open the front door\n",
    "* sell it\n",
    "* paint a wall\n",
    "\n",
    "You could debate these features and these actions, particularly their regional and socioeconomic specificity. Not all houses look like this. It's perhaps better to think of these lists as the template for a certain kind of house rather than all houses. \n",
    "\n",
    "**Object-oriented programming** is a way of organizing your code into patterns like this, separating the qualities of your data (its \"attributes\") from the instructions for things you want to execute on those attributes (its \"methods\"). The result is that, rather than thinking of your code as a directional sequence of events, we are instead thinking about the underlying collections of data and the characteristics that define them. And we arrange the code itself accordingly. To take a more technical example, you might consider an Email object. \n",
    "\n",
    "**Email Object**\n",
    "\n",
    "Attributes\n",
    "\n",
    "* has a sender\n",
    "* has a date\n",
    "* has a body\n",
    "* may have some attachments\n",
    "\n",
    "Methods\n",
    "\n",
    "* can be sent\n",
    "* can be received\n",
    "* can be trashed\n",
    "\n",
    "It is not too difficult to imagine associated pieces of code meant to store these pieces of information or to do each of these particular things. You might have a funtion that defines the sender of an email based on some input, and you might have another that looks to a mail server to send out that note when instructed to do so. Ultimately, thinking in objects allows you to more easily organize text-level and corpus-level functions, is easier to grasp when working at scale, and allows you to store your parameters so they can be imported as a module (a file that contains Python definitions and statements). There are other ways of organizing your code, with their own sets of advantages and disadvantages, but this particular way can often help humanists better grasp what they are working on. \n",
    "\n",
    "To go back to our house example, if the house is the object then a **class** is the blueprint for how one of those objects is built. A class is the template that we write in Python that helps to pull everything together using the attributes and methods we specify. Classes can be as simple or as complex as you want them to be. In the following template, we will define a \"Corpus\" class as well as a \"Text\" class and assign to each class the different attributes we want it to contain and sample methods that might commonly be executed within an NLP project on those attributes. You might want to create your own classes for different use cases. But we find that thinking about corpus and the individual texts within it as distinct texts can be a helpful way to organize things. In the example below, we describe our corpus like so:\n",
    "\n",
    "**Corpus Object**\n",
    "\n",
    "Attributes\n",
    "\n",
    "* has a corresponding directory\n",
    "* has a series of filenames corresponding to the text files contained within that folder\n",
    "* has a list of stopwords associated with it\n",
    "* contains many different Text objects\n",
    "\n",
    "And we describe our texts like so:\n",
    "\n",
    "**Text Object**\n",
    "\n",
    "Attributes\n",
    "\n",
    "* has a filename associated with it\n",
    "* has a raw version of the text\n",
    "* has a tokenized version\n",
    "* has a cleaned tokenized version\n",
    "* has an NLTK version of the text for some quick functionality\n",
    "\n",
    "Methods\n",
    "\n",
    "* the text can be converted from a file into a raw version\n",
    "* the raw version can be tokenized\n",
    "\n",
    "And so on. \n",
    "\n",
    "In what follows below, the two large code blocks contain our classes. This script could be saved as a file in your working directory and updated as neccessary. The subsequent code block allows us to import the script directly into the Python interpreter to play with our classes directly. Working with classes in the way we describe below enables you to move back-and-forth between modifying your code and interacting with it within the interpreter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As mentioned above, this output presents as though it is being run from the command line.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    # rather than enter the data bit by bit, we create a constructor that takes in the data at one time\n",
    "    # all the attributes we want the class to have follow the __init__ syntax\n",
    "    def __init__(self, corpus_dir):\n",
    "        # all the attributes we want the class to have\n",
    "        self.dir = corpus_dir # where corpus_dir is - the corpus' filepath\n",
    "        # classes may contain functions we define ourselves, the all_files() function is defined below\n",
    "        self.filenames = self.all_files()\n",
    "        # this attribute calls nltk's built in English stopwords list and supplements it with punctuation and some extra characters we defined. \n",
    "        self.stopwords = nltk.corpus.stopwords.words('english') + [char for char in string.punctuation] + ['``', \"''\"]\n",
    "        self.texts = [Text(fn, self.stopwords) for fn in self.filenames]\n",
    "\n",
    "    def all_files(self):\n",
    "        \"\"\"given a directory, return the filenames in it\"\"\"\n",
    "        texts = []\n",
    "        for (root, _, files) in os.walk(self.dir):\n",
    "            for fn in files:\n",
    "                print(fn)\n",
    "                if fn[0] == '.': # ignore dot files\n",
    "                    pass\n",
    "                else:\n",
    "                    path = os.path.join(root, fn)\n",
    "                    texts.append(path)\n",
    "        return texts\n",
    "\n",
    "# the Text class works the same as the Corpus, but will contain text-level only attributes\n",
    "class Text(object):\n",
    "    # now create the blueprint for our text object\n",
    "    def __init__(self, fn, stopwords):\n",
    "        # given a filename, store it\n",
    "        self.filename = fn\n",
    "        # a text has raw_text associated with it\n",
    "        self.raw_text = self.get_text()\n",
    "        # a text has raw tokens\n",
    "        self.raw_tokens = nltk.word_tokenize(self.raw_text)\n",
    "        # a text will have a clean version of those tokens\n",
    "        self.cleaned_tokens = self.clean_tokens(stopwords)\n",
    "        # we also want, in this case, to make an NLTK text object\n",
    "        self.nltk_text = nltk.Text(self.cleaned_tokens)\n",
    "        \n",
    "    def get_text(self):\n",
    "        with open(self.filename) as fin:\n",
    "            return fin.read()\n",
    "    \n",
    "    def clean_tokens(self, stopwords):\n",
    "        return [token.lower() for token in self.raw_tokens if token not in stopwords]\n",
    "        \n",
    "# this is what runs if you run the file as a one-off event - $ python3 class_practice.py\n",
    "def main():\n",
    "    corpus_dir = 'corpus/sonnets/'\n",
    "    print('As mentioned above, this output presents as though it is being run from the command line.') # anything that you might want to jump to, such as a graph, FreqDist, etc. would go here\n",
    "\n",
    "# this allows you to import the classes as a module. it uses the special built-in variable __name__ set to the value \"__main__\" if the module is being run as the main program\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payoff of organzing your project within classes is that you can run them as a module from the interpreter or as a Python file from the terminal. For the remainder of this section, we have inserted the above code into a file called file class_practice.py. The following code blocks show how you might go about importing the class and working with it in the terminal. \n",
    "\n",
    "To work with our class in the Python in the interpreter, we first import our script and instantiate our Corpus class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/sonnets/\n",
      "['corpus/sonnets/sonnet_two.txt', 'corpus/sonnets/sonnet_five.txt', 'corpus/sonnets/sonnet_four.txt', 'corpus/sonnets/sonnets_three.txt', 'corpus/sonnets/sonnet_one.txt']\n"
     ]
    }
   ],
   "source": [
    "# import the script as a module--file name without the extension\n",
    "import class_practice \n",
    "\n",
    "# store the path to the corpus directory\n",
    "corpus_dir = \"corpus/sonnets/\"\n",
    "# create a new corpus using our class template\n",
    "this_corpus = class_practice.Corpus(corpus_dir)\n",
    "\n",
    "# now we can access elements of our corpus by accessing this_corpus\n",
    "print(this_corpus.dir) # will show the directory of the corpus\n",
    "print(this_corpus.filenames) # returns all the filenames in the corpus\n",
    "\n",
    "# to work with the text class, instantiate the particular text you want to use\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our corpus is in the interpreter, we can confirm that it contains many texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<class_practice.Text at 0x12a214d90>,\n",
       " <class_practice.Text at 0x10d2e0f10>,\n",
       " <class_practice.Text at 0x12a02f220>,\n",
       " <class_practice.Text at 0x128bc0910>,\n",
       " <class_practice.Text at 0x128bc0bb0>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_corpus.texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a little confusing. As a humanist, we might expect to see the titles of the text or something similar. But we haven't told our class anything like that. Instead, our corpus points to particular texts, represented by their locations in our computer's memory. But, since this is just a list, we can pull out individual texts like you would any other item in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/sonnets/sonnet_two.txt\n",
      "When forty winters shall besiege thy brow,\n",
      "And dig deep trenches in thy beauty's field,\n",
      "Thy youth's proud livery so gazed on now,\n",
      "Will be a totter'd weed of small worth held:\n",
      "Then being asked, where all thy beauty lies,\n",
      "Where all the treasure of thy lusty days;\n",
      "To say, within thine own deep sunken eyes,\n",
      "Were an all-eating shame, and thriftless praise.\n",
      "How much more praise deserv'd thy beauty's use,\n",
      "If thou couldst answer 'This fair child of mine\n",
      "Shall sum my count, and make my old excuse,'\n",
      "Proving his beauty by succession thine!\n",
      "This were to be new made when thou art old,\n",
      "And see thy blood warm when thou feel'st it cold.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "first_text = this_corpus.texts[0]\n",
    "\n",
    "# from here, any of our text level attributes will be available to us:\n",
    "print(first_text.filename)\n",
    "print(first_text.raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could loop over our corpus to pull out information from each text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus/sonnets/sonnet_two.txt\n",
      "corpus/sonnets/sonnet_five.txt\n",
      "corpus/sonnets/sonnet_four.txt\n",
      "corpus/sonnets/sonnets_three.txt\n",
      "corpus/sonnets/sonnet_one.txt\n",
      "When forty winters shall besiege thy bro\n",
      "Those hours, that with gentle work did f\n",
      "Unthrifty loveliness, why dost thou spen\n",
      "Look in thy glass and tell the face thou\n",
      "FROM fairest creatures we desire increas\n"
     ]
    }
   ],
   "source": [
    "for text in this_corpus.texts:\n",
    "    print(text.filename)\n",
    "    \n",
    "# get the first few characters from each line\n",
    "for text in this_corpus.texts:\n",
    "    print(text.raw_text[0:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on how complex you've made your Text class, you can get to some interesting analysis right away. Here, we take advantage of the fact that we use NLTK's more robust Text class to look at the top words in each text. Even though both this small example and NLTK both have created classes named \"Text\", they contain different functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('thy', 7), ('beauty', 4), (\"'s\", 3), ('thou', 3), ('shall', 2), ('and', 2), ('deep', 2), (\"'d\", 2), ('thine', 2), ('praise', 2)]\n",
      "=======\n",
      "[('beauty', 3), ('every', 2), ('doth', 2), ('summer', 2), ('winter', 2), (\"'s\", 2), ('those', 1), ('hours', 1), ('gentle', 1), ('work', 1)]\n",
      "=======\n",
      "[('thy', 6), ('thou', 5), ('dost', 4), ('self', 4), ('thee', 3), ('beauty', 2), (\"'s\", 2), ('nature', 2), ('then', 2), ('canst', 2)]\n",
      "=======\n",
      "[('thou', 6), ('thy', 4), ('glass', 2), ('face', 2), ('time', 2), ('whose', 2), ('mother', 2), ('thee', 2), ('thine', 2), ('look', 1)]\n",
      "=======\n",
      "[('thy', 4), (\"'s\", 3), ('world', 3), ('might', 2), ('but', 2), ('tender', 2), ('thou', 2), ('thine', 2), ('and', 2), ('from', 1)]\n",
      "=======\n"
     ]
    }
   ],
   "source": [
    "for text in this_corpus.texts:\n",
    "    print(text.nltk_text.vocab().most_common(10))\n",
    "    print('=======')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, the process is agnostic of what texts are actually in the corpus folder. So you could use this as a starting point for analysis without having to reinvent the wheel each time. We could, for example, create a new corpus from a different set of texts and quickly grab the most common words from those texts. Let's do this with a small Woolf corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<class_practice.Text object at 0x10d2e0310>, <class_practice.Text object at 0x12a02f910>, <class_practice.Text object at 0x12a2815b0>]\n",
      "corpus/woolf/1922_jacobs_room.txt\n",
      "[('--', 546), ('said', 425), (\"'s\", 411), ('jacob', 390), ('the', 360), ('one', 291), ('i', 236), ('mrs.', 225), ('like', 165), ('but', 153)]\n",
      "======\n",
      "corpus/woolf/1915_the_voyage_out.txt\n",
      "[('i', 1609), (\"'s\", 1007), ('--', 976), ('said', 874), ('one', 801), ('she', 646), ('rachel', 579), ('the', 531), (\"n't\", 513), ('mrs.', 437)]\n",
      "======\n",
      "corpus/woolf/1919_night_and_day.txt\n",
      "[('i', 1967), ('katharine', 1193), (\"'s\", 1139), ('she', 935), ('--', 841), ('said', 796), ('one', 774), (\"n't\", 720), ('he', 615), ('upon', 582)]\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "corpus_dir = \"corpus/woolf/\"\n",
    "new_corpus = class_practice.Corpus(corpus_dir)\n",
    "print(new_corpus.texts)\n",
    "for text in new_corpus.texts:\n",
    "    print(text.filename)\n",
    "    print(text.nltk_text.vocab().most_common(10))\n",
    "    print('======')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to rework all of the basic details of what a corpus looks like. And, looking at these results, we might very quickly notice some changes we want to make to our pipeline! Thanks to how we've organized things, this should not be too challenging. However, this reproducibility is also a potential challenge. Each corpus is different and likely to present its own difficulties. For example, if we wanted to use a TEI-encoded text, this class would not be able to accommodate such a thing. But organizing things with classes means that we could add that to our pipeline fairly easily if we wished. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Note on Making Changes while working in the Terminal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you make changes to your class_practice.py file, it's important to know how these changes will or will not be represented in your working copy of the objects you've created. If, as suggested above, you are working with a class to examine your corpus in the terminal, you must be mindful of one extra step. Once you import your module and create a new instance of your class, any changes to the underlying files for that work will not be represented in the terminal. In order to update your object with new changes, you have to re-import the module into python and re-instantiate your classes. This makes sure you are running the most up-to-date version of your file. You would do that like so, using the above example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "importlib.reload(class_practice)\n",
    "\n",
    "#re-instantiate the corpus or text\n",
    "this_corpus = class_practice.Corpus(corpus_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importlib allows us to reload our module, and we then refresh our this_corpus variable with the most recent version of the Corpus class. Using a class in this way using the terminal can allow you to test particular features of your corpus as you develop your full pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
