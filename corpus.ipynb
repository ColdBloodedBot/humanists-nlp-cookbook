{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with a corpus. This corpus template was taken from the animorphs corpus work. You could get way more involved with it. Might be good to think about what of this is the bare minimum and what could go further. Also note that this won't run in cell with the main piece\n",
    "Q for Brandon: what do you want to keep from here\n",
    "____\n",
    "\n",
    "When working with a corpus of texts it can quickly become confusing to keep track of which step in an NLP pipeline you are on. Say you want to run a Frequency Distribution, did you remeber to tokenize the text? To pull out the stopwords? While this is simple enough if you are working with a small group of texts in a discrete timeperiod, this quickly becomes challenging when working with a large body of texts or when working over a longer period of time. The realities of your project may quickly mean that manually performing each step in your pipeline becomes redundant, hard to keep track of, or a waste of time. This is where classes come in. Utilizing classes allows you to store the qualities of your corpus (it's \"attributes\") and instructions for things you want to execute on those attributes (called \"methods\"). Ultimately, using classes allows you to more easily organize text level and corpus level functions, is easier to grasp when working at scale, and allows you to store your parameters so they can be imported as a module.  \n",
    "\n",
    "Classes can be as simple or as complex as you want them to be. In the following template, we will define a \"Corpus\" and a \"Text\" class and assign to each class the different attributes we want it to contain and sample methods that might commonly be executed within an NLP project on those attributes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    # rather than enter the data bit by bit, we create a constructor that takes in the data at one time \n",
    "    def __init__(self, corpus_dir): # Q for B: do we need \"corpus_dir\" on this line?\n",
    "        # all the attributes we want the class to have\n",
    "        self.dir = corpus_dir # where corpus_dir is the corpus' filepath\n",
    "        # classes may contain functions we define ourselves, the all_files function is defined below\n",
    "        self.filenames = self.all_files()\n",
    "        # this attribute combines multiple parameters (is this the right word?) it calls nltk's built in English stopwords, something built in from string?, and quotation marks\n",
    "        self.stopwords = nltk.corpus.stopwords.words('english') + [char for char in string.punctuation] + ['``', \"''\"]\n",
    "        # for testing limiting to the first few texts\n",
    "        self.texts = [Text(fn, self.stopwords) for fn in self.filenames[0:3]]\n",
    "\n",
    "    def all_files(self):\n",
    "        \"\"\"given the corpus_dir, return the filenames in it\"\"\"\n",
    "        texts = []\n",
    "        for (root, _, files) in os.walk(self.dir):\n",
    "            for fn in files:\n",
    "                path = os.path.join(root, fn)\n",
    "                texts.append(path)\n",
    "        return texts\n",
    "    \n",
    "class Text(object):\n",
    "    def __init__(self, fn, stopwords):\n",
    "        self.filename = fn\n",
    "        self.raw_text = self.get_text()\n",
    "        self.raw_tokens = nltk.word_tokenize(self.raw_text)\n",
    "        self.cleaned_tokens = self.clean_tokens(stopwords)\n",
    "        self.nltk_text = nltk.Text(self.cleaned_tokens)\n",
    "        \n",
    "    def get_text(self):\n",
    "        with open(self.filename) as fin:\n",
    "            return fin.read()\n",
    "    \n",
    "    def clean_tokens(self, stopwords):\n",
    "        return [token.lower() for token in self.raw_tokens if token not in stopwords]\n",
    "        \n",
    "    \n",
    "def main():\n",
    "    corpus_dir = 'corpus/'\n",
    "\n",
    "# this allows you to import the classes as a module\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The payoff of organzing your project within classes is that you can run them as a module from the interpreter. To do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import class_practice \n",
    "# instantiate the Corpus template as class_practice, store as a varaible named this_corpus\n",
    "this_corpus = class_practice.Corpus()\n",
    "# replace \"self\" with \"this_corpus\" to call the methods\n",
    "this_corpus.dir # will show the directory of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you make changes to your class_practice file, you have to re-import it into python 3 and re-instantiate your classes. This makes sure you are running the most up-to-date version of your file. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
