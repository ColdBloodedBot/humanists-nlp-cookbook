{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with plain text files\n",
    "\n",
    "One of the most common formats for working with text files is the .txt format. But there are actually a number of different potential ways to work with one of these files. One of the most basic uses a with statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter I\n",
      "\n",
      "\n",
      "As the streets that lead from the Strand to the Embankment are very\n",
      "narrow, it is better\n"
     ]
    }
   ],
   "source": [
    "filename = 'corpus/1915_the_voyage_out.txt'\n",
    "with open(filename, 'r') as file_in:\n",
    "    text = file_in.read()\n",
    "print(text[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we open the file and assign it a new, temporary name for the duration of the statement. This ensures that the file is opened, dealt with, and then closed safely. Once we un-indent, we have closed the file, and if we tried to read the same file again we would get a ValueError for trying to work with a closed file. The 'as file_in' bit assigns it to a variable so as to help us organize what is happening (we might have another file that we are writing to. Text is now one long string, which is fine in certain cases, but we could also read the contents of it in line by line. Here is a variation on the same approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Chapter I\\n', '\\n', '\\n', 'As the streets that lead from the Strand to the Embankment are very\\n', 'narrow, it is better not to walk down them arm-in-arm. If you persist,\\n', \"lawyers' clerks will have to make flying leaps into the mud; young lady\\n\", 'typists will have to fidget behind you. In the streets of London where\\n', 'beauty goes unregarded, eccentricity must pay the penalty, and it is\\n', 'better not to be very tall, to wear a long blue cloak, or to beat the\\n', 'air with your left hand.\\n']\n"
     ]
    }
   ],
   "source": [
    "filename = 'corpus/1915_the_voyage_out.txt'\n",
    "with open(filename, 'r') as file_in:\n",
    "    text = file_in.readlines()\n",
    "print(text[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"readlines()\" function allows us to take an open file and read it line by line, returning a list of the lines. We assign that list to the text variable here, which we can now use to examine particular parts of the text. Note that here the line breaks do not correspond to sentences. Dividing longer chunks of text into sentences is a separate technique entirely, one called segmentation, that we'll get into later. For now, though, note how this means that the steps required to process your data in the way that you require depend entirely on the way in which it was encoded. In some cases, line breaks can be quite useful, say, when working with poetry where the line breaks are especially meaningful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM fairest creatures we desire increase,\\n', \"That thereby beauty's rose might never die,\\n\", 'But as the riper should by time decease,\\n', 'His tender heir might bear his memory:\\n', 'But thou, contracted to thine own bright eyes,\\n', \"Feed'st thy light'st flame with self-substantial fuel,\\n\", 'Making a famine where abundance lies,\\n', 'Thyself thy foe, to thy sweet self too cruel.\\n', \"Thou that art now the world's fresh ornament\\n\", 'And only herald to the gaudy spring,\\n', 'Within thine own bud buriest thy content\\n', 'And, tender churl, makest waste in niggarding.\\n']\n",
      "=====\n",
      "['Pity the world, or else this glutton be,\\n', \"To eat the world's due, by the grave and thee.\\n\"]\n"
     ]
    }
   ],
   "source": [
    "filename = 'corpus/sonnet_one.txt'\n",
    "with open(filename, 'r') as file_in:\n",
    "    poetry = file_in.readlines()\n",
    "print(poetry[:12])\n",
    "print('=====')\n",
    "print(poetry[12:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling readlines() on a piece of poetry gives us access to the whole poem as a list of lines, so we can manipulate it to chunk the poem into pieces that we care about. Above, I separated the poem into two pieces at the volta, the turn in the sonnet that occurs before the couplet.\n",
    "\n",
    "Those '\\n' characters might appear to be a mistake at first, but worry not! They are actually the computer's representation of a newline character, a way of knowing when a line break happens. Before we process these for analysis, we would want to process those out. One way would be to search each string and remove the character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FROM fairest creatures we desire increase,', \"That thereby beauty's rose might never die,\", 'But as the riper should by time decease,', 'His tender heir might bear his memory:', 'But thou, contracted to thine own bright eyes,', \"Feed'st thy light'st flame with self-substantial fuel,\", 'Making a famine where abundance lies,', 'Thyself thy foe, to thy sweet self too cruel.', \"Thou that art now the world's fresh ornament\", 'And only herald to the gaudy spring,', 'Within thine own bud buriest thy content', 'And, tender churl, makest waste in niggarding.', 'Pity the world, or else this glutton be,', \"To eat the world's due, by the grave and thee.\"]\n"
     ]
    }
   ],
   "source": [
    "cleaned_poem = []\n",
    "for line in poetry:\n",
    "    clean_line = line.replace('\\n', '')\n",
    "    cleaned_poem.append(clean_line)\n",
    "\n",
    "print(cleaned_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This points to an important underlying problem in natural language processing: these texts are not formatted in such a way that they are computer ready right away. That's what puts the natural in natural language! In the case of prose, you can expect a few different categories:\n",
    "\n",
    "1. The text is one continuous string with no line breaks.\n",
    "2. The text has line breaks that correspond to the ends of the lines as laid out on a page.\n",
    "3. The text has line breaks that correspond to meaningful categories.\n",
    "4. Some combination of 2 and 3 (most likely).\n",
    "\n",
    "In most cases, as when working with prose, the line breaks will be used to shape the legibility of a text. Ie - they are meant to assist with the typographical layout, but they have no underlying interpretive meaning. This means that if you wish to preserve the underlying structure of a text you will need to parse the text in a more sophisticated way than just reading it in either as a lump or line by line.\n",
    "\n",
    "Of course, nothing will be as reliable as separating things individually by hand. In the case of a book of poetry, you might, for example, separate each poem into its own text file. The sonnets folder has a set of five Shakespearean sonnets in it. Combining what we've learned already, we can read the filenames from the folder, read them each in, and then store them in a variable for manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "filenames = glob.glob('corpus/sonnets/*.txt')\n",
    "sonnets = []\n",
    "for filename in filenames:\n",
    "    with open(filename, 'r') as file_in:\n",
    "        sonnets.append(file_in.readlines())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list called sonnets, but it's more properly understood as a list of lists, or a list in which each item is itself a list of more items:\n",
    "\n",
    "* List level one: sonnet level.\n",
    "* List level two (sub-list): line level.\n",
    "\n",
    "And we can manipulate this hierarchy to access different elements of the list. This will give us the first sonnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When forty winters shall besiege thy brow,\\n', \"And dig deep trenches in thy beauty's field,\\n\", \"Thy youth's proud livery so gazed on now,\\n\", \"Will be a totter'd weed of small worth held:\\n\", 'Then being asked, where all thy beauty lies,\\n', 'Where all the treasure of thy lusty days;\\n', 'To say, within thine own deep sunken eyes,\\n', 'Were an all-eating shame, and thriftless praise.\\n', \"How much more praise deserv'd thy beauty's use,\\n\", \"If thou couldst answer 'This fair child of mine\\n\", \"Shall sum my count, and make my old excuse,'\\n\", 'Proving his beauty by succession thine!\\n', 'This were to be new made when thou art old,\\n', \"And see thy blood warm when thou feel'st it cold.\\n\"]\n"
     ]
    }
   ],
   "source": [
    "print(sonnets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Unthrifty loveliness, why dost thou spend\\n', \"Upon thy self thy beauty's legacy?\\n\", \"Nature's bequest gives nothing, but doth lend,\\n\", 'And being frank she lends to those are free:\\n', 'Then, beauteous niggard, why dost thou abuse\\n']\n"
     ]
    }
   ],
   "source": [
    "print(sonnets[2][0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organizing things manually like this gives you great control over the files you're working with and their underlying structure. When working with a large corpus you might not have such an option. Separating files by hand is feasible when working with a few texts, but when working with thousands of documents you either have to work with what they give you or develop some computational way of recovering the structure of your text. In these cases, you might rely on textual markers to pinpoint sections of a text.\n",
    "\n",
    "**TODO: Pinpoint by chapter markers.**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
