{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Text Data from Websites\n",
    "\n",
    "Very often there is data on the internet that we would just love to use for our purposes as digital humanists. But, perhaps because it is humanities data, the people publishing it online might not have made it available in a format that is very easily used by you. In a perfect world, everyone would make available clearly described dumps of their data in formats that were usable by machines. In reality, a lot of times people just put things on a web page and call it a day. Web scraping refers to the act of using a computer program to pull down the content of a web page (or, often, many web pages). Scraping is very powerful - once you get the hang of it your potential objects of study will be exponentially increased, as you'll no longer be limited to the data that others make available to you. You can start building your own corpora using real-world information. \n",
    "\n",
    "One of the powerful realizations that comes with doing this kind of work is that text data is all around us. Anything could be a corpus with enough time and attention. The internet, in particular, offers a wealth of opportunities for aquiring text data if we know how to get at it. This could come in many forms:\n",
    "\n",
    "* Data that exists openly on the internet but that has not been prepared for easy use.\n",
    "* Data that exists openly on the internet and that has been provided in a usable form.\n",
    "\n",
    "Let's start with the former use case. Most often, data on the internet is not presented in a form that is easily accessible. This might be because the author of a particular webpage was not expecting the site to be read and interpreted by anything other than humans, or it might be that the particular form in which the data is presented is not the best form for your purposes. In either case, the ability to extract textual information from a website can be quite powerful.\n",
    "\n",
    "Texts drawn from http://www.gutenberg.org/files/12242/12242-h/12242-h.htm\n",
    "\n",
    "In the following examples we will be using the Beautiful Soup package. First, some initial setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages for webscraping.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "\n",
    "# store the url we want to work with in the variable 'url'\n",
    "\n",
    "url = 'https://github.com/walshbr/humanists-nlp-cookbook/blob/master/scraping-corpus/dickinson/xi.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the website at that URL. You will find a text by Emily Dickinson surrounded by the web interface for GitHub. If we wanted to use the text of that poem in a particular program, we could just copy it manually to a text editor. But what if we had ten different poems on different pages? A hundred? At scale this quickly becomes something that we might want a computer to do for us. That's where scraping comes in. Let's use Python to pull in the HTML of that page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\n\\n\\n\\n\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://assets-cdn.github.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars0.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars1.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars2.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars3.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\\n\\n\\n\\n  <link crossorigin=\"anonymous\" href=\"https://assets-cdn.github.com/assets/frameworks-98cac35b43fab8341490a2623fdaa7b696bbaea87bccf8f485fd5cdb4996cd9b52bdb24709fb3bab0a0dcff4a29187d65028ee693d609ce5c0c3283c77a247a9.css\" media=\"all\" rel=\"stylesheet\" />\\n  <link crossorigin=\"anonymous\" href=\"https://assets-cdn.github.com/assets/github-2909a5c7c333104e2fc3814a4565e315d2348d0d4a1f16c16ff32cc32006996950b78407dcf1'\n"
     ]
    }
   ],
   "source": [
    "html = request.urlopen(url).read()\n",
    "print(html[0:1000])\n",
    "soup = BeautifulSoup(html, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've ever worked with HTML before this should look familiar. It looks a bit wonky becaue Python is reading it as one long string, devoid of any helpful tabbing and whitespace. If you haven't, never fear! Let's take a moment to talk about what you're seeing.\n",
    "\n",
    "Brief introduction to HTML\n",
    "\n",
    "Beautiful Soup offers us an easy way to interact with the HTML, pull particular pieces out, and extract what we need. The previous lines say, \"take the HTML that you've pulled down and get ready to do Beautiful Soup things to it.\" Think of it this way: you have a certain number of things that you can do in your car:\n",
    "    \n",
    "* Drive\n",
    "* Fill it with gas\n",
    "* Change the tires\n",
    "    \n",
    "But you can only really do those things once you actually get in your car. You couldn't change your tires if you were riding a horse. Horses don't have wheels. In programming speak, we're saying \"turn that HTML into a Beautiful Soup **object**.\" Saying something is an object is a way of saying \"I expect this data to have certain characteristics and be able to do certain things.\" In this case, BeautifulSoup gives us a series of ways to manipulate the HTML using HTML and CSS structural elements. We can do things like:\n",
    "\n",
    "* Get all the links\n",
    "* Get all the text on a page\n",
    "\n",
    "The sky is the limit, and you can use HTML and CSS to drill down into the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      \n",
      "        \n",
      "        XI.\n",
      "      \n",
      "      \n",
      "        \n",
      "        \n",
      "\n",
      "      \n",
      "      \n",
      "        \n",
      "        THE OUTLET.\n",
      "      \n",
      "      \n",
      "        \n",
      "        \n",
      "\n",
      "      \n",
      "      \n",
      "        \n",
      "        My river runs to thee:\n",
      "      \n",
      "      \n",
      "        \n",
      "        Blue sea, wilt welcome me?\n",
      "      \n",
      "      \n",
      "        \n",
      "        \n",
      "\n",
      "      \n",
      "      \n",
      "        \n",
      "        My river waits reply.\n",
      "      \n",
      "      \n",
      "        \n",
      "        Oh sea, look graciously!\n",
      "      \n",
      "      \n",
      "        \n",
      "        \n",
      "\n",
      "      \n",
      "      \n",
      "        \n",
      "        I'll fetch thee brooks\n",
      "      \n",
      "      \n",
      "        \n",
      "        From spotted nooks, —\n",
      "      \n",
      "      \n",
      "        \n",
      "        \n",
      "\n",
      "      \n",
      "      \n",
      "        \n",
      "        Say, sea,\n",
      "      \n",
      "      \n",
      "        \n",
      "        Take me!\n",
      "      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the html and the soup\n",
    "\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "# get the HTML using the particular class structure\n",
    "poem_html = soup.select(\".blob-wrapper.data.type-text table\")[0]\n",
    "print(poem_html.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, poem_html grabs the HTML we care about, and Beautiful Soup gives us access to the get_text() command. This powerful method allows us to throw away all the HTML tags and focus only on the text itself.\n",
    "\n",
    "That's the text of the poem, but we'll need to do a bit more to make this workable data. All the whitespace that makes the text appear neatly on the page looks a little bizarre when pulled into Python. And, apart from aesthetics, it will cause problems with processing. We can strip that whitespace out, because those line breaks are actually represented as characters in the data itself, as '\\n'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         XI.                                                                 THE OUTLET.                                                                 My river runs to thee:                                Blue sea, wilt welcome me?                                                                 My river waits reply.                                Oh sea, look graciously!                                                                 I'll fetch thee brooks                                From spotted nooks, —                                                                 Say, sea,                                Take me!        \n"
     ]
    }
   ],
   "source": [
    "poem_text = poem_html.get_text()\n",
    "clean_poem = poem_text.replace('\\n', ' ')\n",
    "print(clean_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gets us closer - the .replace() method takes two arguments, the first of which is the thing we're looking to replace and that second of which is the thing to replace it with. The approach works here, but note how we _still_ have a lot of whitespace! That's because we have so many \\n characters to replace that we've simply removed one problem and replaced it with another. We will discuss more of these sorts of issues more in the section on data cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out how to get to the data you want, the first step is to check out the HTML behind the website you're interested in. Modern web browsers have tools to help facilitate this. In Chrome, for example, at the time of writing you can right click on a part of a website and select ['inspect element'](https://developers.google.com/web/tools/chrome-devtools/inspect-styles/) to expose the underlying HTML of a page. This will show the HTML, that stuff that BeautifulSoup can help you work through. From there, knowledge of [HTML](https://www.w3schools.com/html/) and [CSS](https://www.w3schools.com/css/) can help you select particular pieces of the page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a List of Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to tell if a website is good for scraping re: URL construction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: scraping sources through a script like this can raise a lot of questions. Do the people allow you to do so? Some websites explicitly detail whether or not you can in their terms of service. Project Gutenberg, for example, explicitly tells you that you *cannot* scrape their website. Doing so anyway potentially opens you to legal repercussions. Even if a site does not explicitly forbid scraping, it can still feel ethically suspect. A recent example of this is when a research scraping all publically available OKCupid user data. While it is true that these users made their personal information publicly available, they probably did not intend that their lives be exposed to this level of scrutiny. When getting ready to scrape data, it's usually a good idea to ask a series of questions:\n",
    "* Was this data meant to be public?\n",
    "* Am I harming anyone by pulling down this data?\n",
    "* Is this data associated with anyone's identity in a way that they might object to?\n",
    "* Is it worth it?\n",
    "* Can I get the data in some other way?\n",
    "* Is my scraping going to harm the website in some way?\n",
    "\n",
    "more on terms of service?\n",
    "\n",
    "Related to this last point - even if all these questions seem to be fine, you still need to be careful. Scraping a website can very often look like a [DDoS attack](https://en.wikipedia.org/wiki/Denial-of-service_attack). If you, say, try to scrape 10,000 links from Project Gutenberg, those 10,000 hits on Project Gutenberg's site could cause issues for their system. To get around this, it's often good practice to purposely slow down your scraper so that it more closely mimics the behavior of a human user. Rather than scraping multiple links per second, the following snippet tells the scraper to rest a random interval of up to 6 seconds between downloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "def download(url, sleep=True):\n",
    "    if sleep:\n",
    "        time.sleep(random.random() * max_sleep)\n",
    "    html = request.urlopen(url).read().decode('utf8', errors='replace')\n",
    "    return BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everytime you call the \"download()\" function, then, it would sleep a randoml amount of time. \n",
    "\n",
    "If you're really concerned it is usually a good idea to contact the people whose site you want to work with to ask if they mind you scraping their work. Sometimes they might make their data available in a more usable way. If you work at an institution with an IRB panel, they can probably help you make determinations about whether the data involved with your work is sensitive if it involves human subejcts. more on sleep?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
