{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting Text Data from Websites\n",
    "\n",
    "One of the powerful realizations that comes with doing this kind of work is that text data is all around us. Very often there is data on the internet that we would just love to use for our purposes as digital humanists. In a perfect world, everyone would make available clearly described dumps of their data in formats that were usable by machines. In reality, often people just put things on a web page and call it a day. Most often, data on the internet is not presented in a form that is easily accessible.\n",
    "\n",
    "Web scraping refers to the act of using a computer program to pull down the content of a web page (or, often, many web pages). Scraping is very powerful, and it can often be the first step in a pipeline that takes information from the web and prepares it for use in NLP. Once you get the hang of scraping, a range of new objects of study will become accessible to you. You'll no longer be limited to the data that others make available to you. You can start building your own corpora using real-world information. \n",
    "\n",
    "Anything could be a corpus with enough time, attention, and thought. The web, in particular, offers a wealth of opportunities for aquiring text data if we know how to get at it. In the following examples we will be using the Beautiful Soup package to work with Emily Dickinson poems drawn from [Project Gutenberg](http://www.gutenberg.org/files/12242/12242-h/12242-h.htm). First, some initial setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages for webscraping.\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import request\n",
    "\n",
    "# store the url we want to work with in the variable 'url'\n",
    "\n",
    "url = 'https://github.com/walshbr/humanists-nlp-cookbook/blob/master/scraping-corpus/dickinson/xi.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the website at that URL. You will find a text by Emily Dickinson surrounded by the web interface for GitHub. If we wanted to use the text of that poem in a particular program, we could just copy it manually to a text editor. But what if we had ten different poems on different pages? A hundred? At scale this quickly becomes something that we might want a computer to do for us. That's where scraping comes in. Let's use Python to pull in the HTML of that page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\n\\n\\n\\n\\n<!DOCTYPE html>\\n<html lang=\"en\">\\n  <head>\\n    <meta charset=\"utf-8\">\\n  <link rel=\"dns-prefetch\" href=\"https://github.githubassets.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars0.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars1.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars2.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://avatars3.githubusercontent.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://github-cloud.s3.amazonaws.com\">\\n  <link rel=\"dns-prefetch\" href=\"https://user-images.githubusercontent.com/\">\\n\\n\\n\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-jFUBCdWOA1Ov3xo3oFMBwsdP4Up2K1bRnP4QYI5WqvpaIYxWVek89k2M0oyTbNhYMViGtxJB3Vdwcw8ln8hGQw==\" rel=\"stylesheet\" href=\"https://github.githubassets.com/assets/frameworks-8c550109d58e0353afdf1a37a05301c2.css\" />\\n  <link crossorigin=\"anonymous\" media=\"all\" integrity=\"sha512-fqnZtayqgLCmcQfxXp5OH4orKvv16fP0zCU6Ns+NuAULztoXdSMDcdECjHWSFA018nYJNqWh1OXhgZ9W8'\n"
     ]
    }
   ],
   "source": [
    "html = request.urlopen(url).read()\n",
    "print(html[0:1000])\n",
    "soup = BeautifulSoup(html, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you've ever worked with HTML before this should look familiar. It looks a bit wonky becaue Python is reading it as one long string, devoid of any helpful tabbing and whitespace. If you haven't, you might check out the Mozilla [introduction to HTML](https://developer.mozilla.org/en-US/docs/Learn/HTML/Introduction_to_HTML) before reading further. For the purposes of this lesson, you mostly just need to know that any given web page has two distinct forms - the version you see and the version that the browser analyzes to present the version you see. So you might see this:\n",
    "\n",
    "This is a paragraph that you see in the browser.\n",
    "\n",
    "But the browser, behind the scenes, sees this:\n",
    "\n",
    "`<p>This is a paragraph that you see in the browser.</p>`\n",
    "\n",
    "The `<p></p>` tag pieces get stripped away by your browser, but, in the process, they tell it to present the enclosed text as a paragraph. The collected structuring system of encoding this represents is supplemented by a companion system called CSS, which is used to give the page the design and aesthetics to make it something other than just plain text and images on a page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML and CSS represent the structure and design of any given webpage. Beautiful Soup is a Python library that offers us an easy way to interact with the HTML, pull particular pieces out, and extract what we need. The previous lines of code say, \"take the HTML that you've pulled down and get ready to do Beautiful Soup things to it.\" Think of it this way: you have a certain number of things that you can do in your car:\n",
    "    \n",
    "* Drive\n",
    "* Fill it with gas\n",
    "* Change the tires\n",
    "    \n",
    "But you can only really do those things once you actually start interacting with a car. You couldn't change your tires if you were riding a horse. Horses don't have wheels. In programming speak, we're saying \"turn that HTML into a Beautiful Soup **object**.\" Saying something is an object is a way of saying \"I expect this data to have certain characteristics and be able to do certain things.\" In this case, BeautifulSoup gives us a series of ways to interact with a website using its associated HTML and CSS structural elements. We can do things like:\n",
    "\n",
    "* Get all the links\n",
    "* Get all the text on a page\n",
    "\n",
    "First let's get the text of just one portion of the webpage we're looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      \n",
      "        \n",
      "        XI.\n",
      "      \n",
      "      \n",
      "        \n",
      "        \n",
      "\n",
      "      \n",
      "      \n",
      "        \n",
      "        THE OUTLET.\n",
      "      \n",
      "      \n",
      "        \n",
      "        \n",
      "\n",
      "      \n",
      "      \n",
      "        \n",
      "        My river runs to thee:\n",
      "      \n",
      "      \n",
      "        \n",
      "        Blue sea, wilt welcome me?\n",
      "      \n",
      "      \n",
      "        \n",
      "        \n",
      "\n",
      "      \n",
      "      \n",
      "        \n",
      "        My river waits reply.\n",
      "      \n",
      "      \n",
      "        \n",
      "        Oh sea, look graciously!\n",
      "      \n",
      "      \n",
      "        \n",
      "        \n",
      "\n",
      "      \n",
      "      \n",
      "        \n",
      "        I'll fetch thee brooks\n",
      "      \n",
      "      \n",
      "        \n",
      "        From spotted nooks, â€”\n",
      "      \n",
      "      \n",
      "        \n",
      "        \n",
      "\n",
      "      \n",
      "      \n",
      "        \n",
      "        Say, sea,\n",
      "      \n",
      "      \n",
      "        \n",
      "        Take me!\n",
      "      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the html and the soup\n",
    "\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html, 'html5lib')\n",
    "\n",
    "# get the HTML using the particular class structure\n",
    "poem_html = soup.select(\".blob-wrapper.data.type-text table\")[0]\n",
    "print(poem_html.get_text())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, poem_html grabs the HTML we care about, and Beautiful Soup gives us access to the get_text() command. This powerful method allows us to throw away all the HTML tags and focus only on the text itself. Often the most challenging part with scraping is knowing which selectors to use - the \".blob-wrapper.data.type-text table\" part of the above code. \n",
    "\n",
    "To find out how to get to the data you want, the first step is to check out the HTML behind the website you're interested in. Modern web browsers have tools to help facilitate this. In Chrome, for example, at the time of writing you can right click on a part of a website and select ['inspect element'](https://developers.google.com/web/tools/chrome-devtools/inspect-styles/) to expose the underlying HTML of a page. This will show the HTML, that stuff that BeautifulSoup can help you work through. From there, knowledge of [HTML](https://www.w3schools.com/html/) and [CSS](https://www.w3schools.com/css/) can help you select particular pieces of the page. \n",
    "\n",
    "So far we have the text of the poem, but we'll need to do a bit more to make this workable data. All the whitespace that makes the text appear neatly on the page looks a little bizarre when pulled into Python. And, apart from aesthetics, it will cause problems with processing. We can strip that whitespace out, because those line breaks are actually represented as characters in the data itself, as '\\n'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         XI.                                                                 THE OUTLET.                                                                 My river runs to thee:                                Blue sea, wilt welcome me?                                                                 My river waits reply.                                Oh sea, look graciously!                                                                 I'll fetch thee brooks                                From spotted nooks, â€”                                                                 Say, sea,                                Take me!        \n"
     ]
    }
   ],
   "source": [
    "poem_text = poem_html.get_text()\n",
    "clean_poem = poem_text.replace('\\n', ' ')\n",
    "print(clean_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gets us closer - the .replace() method takes two arguments, the first of which is the thing we're looking to replace and that second of which is the thing to replace it with. The approach works here, but note how we _still_ have a lot of whitespace! That's because we have so many \\n characters to replace that we've simply removed one problem and replaced it with another. We'd need to clean this more in order to use it, and we might want to be more careful, given that we're looking at a poem where whitespace and linebreaks are meaningful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['XI.', '\\n', 'THE OUTLET.', '\\n', 'My river runs to thee:', 'Blue sea, wilt welcome me?', '\\n', 'My river waits reply.', 'Oh sea, look graciously!', '\\n', \"I'll fetch thee brooks\", 'From spotted nooks, â€”', '\\n', 'Say, sea,', 'Take me!']\n"
     ]
    }
   ],
   "source": [
    "text_of_the_poem = [td.text for td in poem_html.find_all('td') if td.text != '']\n",
    "print(text_of_the_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous code example, we return to the html of the poem and look for all 'td' tags (data cells). If the text of teh cell is empty, we throw that example away. And we reconstitute what we get in return into a list. This actually gets us pretty close to what we might want. We have the poem represented as a list, with each line as an element in that list. And the `\\n` line breaks in the list represent stanza breaks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping a Series of Pages\n",
    "\n",
    "Scraping often follows a pretty standard process:\n",
    "\n",
    "* get a scraper working for one page\n",
    "* gather a list of all the related pages\n",
    "* apply the scraper to all the pages at once\n",
    "\n",
    "In general terms, this means that your first big task is simply to get a list of all the URLs you are interested in scraping. In this example, we have a series of texts available on GitHub [here](https://github.com/walshbr/humanists-nlp-cookbook/tree/master/scraping-corpus/dickinson). We'll scrape this site and manipulate the results to get the list we need for the final scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"js-navigation-open link-gray-dark\" href=\"/walshbr/humanists-nlp-cookbook/blob/master/scraping-corpus/dickinson/xi.txt\" id=\"144560cfc44322009bd525f9587e622b-17349cc6366e2bb355a8c89bd47a9f4d6ee03add\" title=\"xi.txt\">xi.txt</a>\n",
      "/walshbr/humanists-nlp-cookbook/blob/master/scraping-corpus/dickinson/xi.txt\n",
      "https://github.com/walshbr/humanists-nlp-cookbook/blob/master/scraping-corpus/dickinson/xi.txt\n",
      "https://github.com/walshbr/humanists-nlp-cookbook/blob/master/scraping-corpus/dickinson/xi.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n\\nXI.\\n',\n",
       " '\\n\\n\\n\\n',\n",
       " '\\n\\nTHE OUTLET.\\n',\n",
       " '\\n\\n\\n\\n',\n",
       " '\\n\\nMy river runs to thee:\\n',\n",
       " '\\n\\nBlue sea, wilt welcome me?\\n',\n",
       " '\\n\\n\\n\\n',\n",
       " '\\n\\nMy river waits reply.\\n',\n",
       " '\\n\\nOh sea, look graciously!\\n',\n",
       " '\\n\\n\\n\\n',\n",
       " \"\\n\\nI'll fetch thee brooks\\n\",\n",
       " '\\n\\nFrom spotted nooks, â€”\\n',\n",
       " '\\n\\n\\n\\n',\n",
       " '\\n\\nSay, sea,\\n',\n",
       " '\\n\\nTake me!\\n']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://github.com/walshbr/humanists-nlp-cookbook/tree/master/scraping-corpus/dickinson'\n",
    "html = request.urlopen(url).read()\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "# get the HTML using the particular class structure of that page\n",
    "links = soup.find_all('a', class_='js-navigation-open link-gray-dark')\n",
    "\n",
    "# now we have the links, but we need to manipulate them to pull out the information we want. We mostly want the 'href' attribute\n",
    "print(links[0])\n",
    "\n",
    "links = [link['href'] for link in links]\n",
    "\n",
    "# notice that the links are missing the top-level domain, which we need for them to resolve. \n",
    "print(links[0])\n",
    "# let's go through and add the domain.\n",
    "links = ['https://github.com' + link for link in links]\n",
    "print(links[0])\n",
    "\n",
    "# we now have a list of links, and we could loop over them to scrape each poem's text. \n",
    "\n",
    "poems = []\n",
    "for link in links:\n",
    "    html = request.urlopen(link).read()\n",
    "    soup = BeautifulSoup(html)\n",
    "    lines = soup.find_all('tr')\n",
    "    this_poem = [line.text for line in lines]\n",
    "    poems.append(this_poem)\n",
    "\n",
    "# lots of extra line breaks, as this is scraped from the web, but it's there!\n",
    "poems[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This brings up a related point - not all websites are equally good candidates for scraping. In the above example, we used the main github page for the [file](https://raw.githubusercontent.com/walshbr/humanists-nlp-cookbook/master/scraping-corpus/dickinson/xi.txt), because that was easily accessible to us. The poem is there, but it is embedded in a viewing interface designed by GitHub. And the complicated set of scraping we did involving the 'tr' tags was meant to get rid of that interface and pull out just the poem. GitHub also offers a [raw link](https://raw.githubusercontent.com/walshbr/humanists-nlp-cookbook/master/scraping-corpus/dickinson/xi.txt) for each of these files that is separate from the viewing interface. We could have scraped that directly for cleaner results. In order to do so we would have had to manipulate our URLS to change this\n",
    "\n",
    "https://github.com/walshbr/humanists-nlp-cookbook/blob/master/scraping-corpus/dickinson/xi.txt\n",
    "\n",
    "to this\n",
    "\n",
    "https://raw.githubusercontent.com/walshbr/humanists-nlp-cookbook/master/scraping-corpus/dickinson/xi.txt\n",
    "\n",
    "Definitely doable! And we would have used string methods to transform our initial URL into the raw version.\n",
    "\n",
    "So this was a slightly complicated example to use, but it still works. When deciding whether or not scraping is an option, at its core, you are looking for a site that has a repeated structure across all its pages. You can ask a short set of questions of a page to determine if scraping is an option:\n",
    "\n",
    "* are all the links to the pages you're interested in present on a single page?\n",
    "* are all the pages you're interested in, instead, represented on a series of pages?\n",
    "* are the urls formed in a consistent manner that you could extrapolate? For example, site.com/posts/1; site.com/posts/2\n",
    "\n",
    "In general, sites in these forms are the easiest to pull data from, because you can easily imagine constructing the URLs, collecting them, and harvesting them. If you need, say, to input a search query into a form on the page to get the results you would want to scrape the process gets much more complicated if not impossible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping sources through a script like this can raise a lot of questions. Do the people allow you to do so? Some websites explicitly detail whether or not you can scrape in their terms of service. Project Gutenberg, for example, explicitly tells you that you *cannot* scrape their website. Doing so anyway potentially opens you to legal repercussions. Even if a site does not explicitly forbid scraping, it can still feel ethically suspect to do so. A recent example of this is when a researcher scraped all publically available OKCupid user data and made it available online. The mere presence of the data online does not mean it was meant to be used for scraping. When getting ready to scrape data, it's usually a good idea to ask a series of questions:\n",
    "* Was this data meant to be public?\n",
    "* Am I harming anyone by pulling down this data?\n",
    "* Is this data associated with anyone's identity in a way that they might object to?\n",
    "* Is it worth it?\n",
    "* Can I get the data in some other way?\n",
    "* Is my scraping going to harm the website in some way?\n",
    "\n",
    "Related to this last point - even if all these questions seem to be fine, you still need to be careful. Even if scraping is allowed, doing so rapidly can very often look like a [DDoS attack](https://en.wikipedia.org/wiki/Denial-of-service_attack). If you, say, try to scrape 10,000 links a single site, those 10,000 hits on could cause issues for their system. To get around this, it's often good practice to purposely slow down your scraper so that it more closely mimics the interactions of a human user. Rather than scraping multiple links per second, the following snippet tells the scraper to rest a random interval of up to 6 seconds between downloads:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "def download(url, sleep=True):\n",
    "    if sleep:\n",
    "        time.sleep(random.random() * max_sleep)\n",
    "    html = request.urlopen(url).read().decode('utf8', errors='replace')\n",
    "    return BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everytime you call the \"download()\" function, then, it would sleep a randoml amount of time. \n",
    "\n",
    "If you're concerned it is usually a good idea to contact the people whose site you want to work with to ask if they mind you scraping their work. Sometimes they might welcome your efforts to make their data available in a more usable way. And if you work at an institution with an IRB panel, they can probably help you make determinations about whether the data involved with your work is sensitive if it involves human subejcts. It's better to approach these questions intentionally and with care."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
