{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords and Filtering\n",
    "\n",
    "## Using stopwords\n",
    "\n",
    "In natural language processing, we frequently work with a list of stopwords, those words that occur most often in any given text in a language. NLTK has a handy way for pulling all those stopwords into your program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words here are probably unsurprising: prepositions, pronouns, and similar words that show up frequently across English-language corpora. In most cases you will want to supplement such a list with your own words to create a stopwords list that makes sense for your corpus. If you work with early modern texts, for example, you probably have a whole range of different words and phrases particular to your period that you would want to keep in mind. NLTK comes with a range of different lists for different languages, but there are a range of other options available [online ](https://github.com/Alir3z4/stop-words). [CLTK](http://cltk.org/), a variation of NLTK for working with ancient languages, comes with its own lists as well. \n",
    "\n",
    "It's common in NLP to use lists like these to create filter your text. Let's take a piece of the first chunk of Jacob's room, using the get_chunks() method we developing in the unit on [chunking](chunking.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER',\n",
       " 'ONE',\n",
       " \"''\",\n",
       " 'So',\n",
       " 'of',\n",
       " 'course',\n",
       " ',',\n",
       " \"''\",\n",
       " 'wrote',\n",
       " 'Betty',\n",
       " 'Flanders',\n",
       " ',',\n",
       " 'pressing',\n",
       " 'her',\n",
       " 'heels',\n",
       " 'rather',\n",
       " 'deeper',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sand',\n",
       " ',',\n",
       " '``',\n",
       " 'there',\n",
       " 'was',\n",
       " 'nothing',\n",
       " 'for',\n",
       " 'it',\n",
       " 'but',\n",
       " 'to',\n",
       " 'leave',\n",
       " '.',\n",
       " \"''\",\n",
       " 'Slowly',\n",
       " 'welling',\n",
       " 'from',\n",
       " 'the',\n",
       " 'point',\n",
       " 'of',\n",
       " 'her',\n",
       " 'gold',\n",
       " 'nib',\n",
       " ',',\n",
       " 'pale',\n",
       " 'blue',\n",
       " 'ink',\n",
       " 'dissolved',\n",
       " 'the',\n",
       " 'full',\n",
       " 'stop',\n",
       " ';']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "\n",
    "def get_chunks(text, num_chunks):\n",
    "    text_length = len(text)\n",
    "    text_chunks = []\n",
    "    number_of_chunks = num_chunks\n",
    "    for i in range(number_of_chunks):\n",
    "        chunk_size = text_length/number_of_chunks\n",
    "        chunk_start = math.floor(chunk_size * i)\n",
    "        chunk_end = math.floor(chunk_size * (i +1))\n",
    "        text_chunks.append(text[chunk_start:chunk_end])\n",
    "    return text_chunks\n",
    "\n",
    "filename = 'corpus/1922_jacobs_room.txt'\n",
    "with open(filename, 'r') as fin:\n",
    "    raw_text = fin.read()\n",
    "\n",
    "chunked_text = get_chunks(raw_text, 100)\n",
    "tokenized_text = [nltk.word_tokenize(chunk) for chunk in chunked_text]\n",
    "tokenized_text[0][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our stopwords list from above to filter out common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHAPTER', 'ONE', \"''\", 'So', 'course', ',', \"''\", 'wrote', 'Betty', 'Flanders', ',', 'pressing', 'heels', 'rather', 'deeper', 'sand', ',', '``', 'nothing', 'leave', '.', \"''\", 'Slowly', 'welling', 'point', 'gold', 'nib', ',', 'pale', 'blue', 'ink', 'dissolved', 'full', 'stop', ';']\n"
     ]
    }
   ],
   "source": [
    "filtered_chunk = [token for token in tokenized_text[0][:50] if token not in nltk.corpus.stopwords.words('english')]\n",
    "print(filtered_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the words that are gone now: 'of', 'her', 'in', 'the'. Filtering words like this often gives us a sense of those words that are more likely to be meaningful for the particular text. Here, we get character names, as well as a sense of vocabulary that might be thought of as more particular to Woolf. Note how these words are particular to the content of her text - we get a sense of what she is writing about and how she is describing it. You could imagine comparing this list of vocabulary to another author and finding it to be quite different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding to your stopwords list\n",
    "\n",
    "In the previous example, even though we filtered out common words, we still have quite a lot of unwanted characters - punctuation. These grammatical markings are often filtered out in much the same way, using a different part of nltk. There are a couple different ways to do this. The simplest draws upon Python's built-in string class, which has a list of punctuation in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "\n",
    "# create an initial stopword list by loading in the one from nltk\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# add punctuation to that list and print out our new list\n",
    "\n",
    "stopword_list.extend(string.punctuation)\n",
    "print(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHAPTER', 'ONE', \"''\", 'So', 'course', \"''\", 'wrote', 'Betty', 'Flanders', 'pressing', 'heels', 'rather', 'deeper', 'sand', '``', 'nothing', 'leave', \"''\", 'Slowly', 'welling', 'point', 'gold', 'nib', 'pale', 'blue', 'ink', 'dissolved', 'full', 'stop']\n"
     ]
    }
   ],
   "source": [
    "filtered_chunk = [token for token in tokenized_text[0][:50] if token not in stopword_list]\n",
    "print(filtered_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gets us closer - we pulled out a few punctuation marks. But notice that some still made it through. So we'll want to further extend our stopwords list based on the things that got left out. You can use the same approach to add corpus-specific stopwords if you need to do so. Your research questions will ultimately drive your decisions about what words to include or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHAPTER', 'ONE', 'So', 'course', 'wrote', 'Betty', 'Flanders', 'pressing', 'heels', 'rather', 'deeper', 'sand', 'nothing', 'leave', 'Slowly', 'welling', 'point', 'gold', 'nib', 'pale', 'blue', 'ink', 'dissolved', 'full', 'stop']\n"
     ]
    }
   ],
   "source": [
    "# make a custom stopwords list and add it to our general stopwords list.\n",
    "custom_stop_list = [\"''\", \"``\"]\n",
    "stopword_list.extend(custom_stop_list)\n",
    "filtered_chunk = [token for token in tokenized_text[0][:50] if token not in stopword_list]\n",
    "print(filtered_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks more like it. We've successfully massaged out all the punctuation in this initial chunk, though further situations like this might come up later on. The lesson here is that textual data is _messy_, and even the most sophisticated natural language processing setups require a good deal of massaging and careful modification in light of particular situations.  Each text presents its own set of problems, and only through familiarity with your objects of study will you know what exactly needs to be accounted for. In short, there is no substitute for knowing your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why you might want to leave stopwords in\n",
    "\n",
    "There are good reasons why you might actually want to leave those stopwords in for your analysis. If these words are statistically the most common in English-language texts, then they must serve as meaningful points of comparison among many different texts. Put another way, any given two authors might take different objects of study in their texts. Filtering out common words can give a better sense of their particular interests. But the common words that they share might give a good sense of the particulars of their own literary style, of the ways in which they write. Let's use this principle to compare Jacob's Room with The Voyage Out. Even though both these texts are by Virginia Woolf, they have very different styles. Let's see if these are represented by word counts. In what follows, we'll compare the texts in two ways, first with stopwords taken out and second using _only_ the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====\n",
      "Comparison of Texts with Stopwords Excluded\n",
      "The Voyage Out\n",
      "[(\"'s\", 1007), ('said', 874), ('one', 801), ('rachel', 579), (\"n't\", 513), ('mrs.', 437), ('like', 392), ('helen', 392), ('could', 380), ('people', 379)]\n",
      "Jacob's Room\n",
      "[('said', 425), (\"'s\", 411), ('jacob', 390), ('one', 291), ('mrs.', 225), ('like', 165), ('would', 150), ('little', 137), ('mr.', 134), ('flanders', 128)]\n",
      "=====\n",
      "Comparison of Stopwords in the texts\n",
      "The Voyage Out\n",
      "[(',', 10332), ('the', 7209), ('.', 7193), ('and', 4478), ('of', 3723), ('to', 3640), ('a', 2980), (\"''\", 2866), ('``', 2590), ('she', 2548), ('in', 2249), ('was', 2113), ('her', 1979), ('he', 1722), ('that', 1667)]\n",
      "Jacob's Room\n",
      "[(',', 4604), ('the', 3920), ('.', 2964), ('and', 1816), ('of', 1330), ('a', 1144), ('to', 1016), ('in', 997), (\"''\", 939), ('``', 805), (';', 790), ('was', 686), ('her', 605), ('he', 582), ('it', 574)]\n"
     ]
    }
   ],
   "source": [
    "# store the filenames of both texts\n",
    "\n",
    "fn_one = 'corpus/1915_the_voyage_out.txt'\n",
    "fn_two = 'corpus/1922_jacobs_room.txt'\n",
    "\n",
    "# read in the text of the_voyage_out and assign it a variable text_voyage\n",
    "\n",
    "with open(fn_one, 'r') as fin:\n",
    "    text_voyage = fin.read()\n",
    "\n",
    "# read in the text of jacobs_room and assign it a variable text_jacob\n",
    "    \n",
    "with open(fn_two, 'r') as fin:\n",
    "    text_jacob = fin.read()\n",
    "\n",
    "# read in the English-language stopwords list provided by nltk and store it in \n",
    "# a variable stopword_list\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.extend(string.punctuation)\n",
    "custom_stop_list = [\"''\", \"``\", '``','--']\n",
    "stopword_list.extend(custom_stop_list)\n",
    "# use nltk's built-in tokenizer to get a list of the tokens in each text. store\n",
    "# these in variables.\n",
    "\n",
    "tokens_voyage = [token.lower() for token in nltk.word_tokenize(text_voyage)]\n",
    "tokens_jacob = [token.lower() for token in nltk.word_tokenize(text_jacob)]\n",
    "\n",
    "\n",
    "# look at the tokens we've got and filter out stopwords by comparing tokens against\n",
    "# the stored list of stopwords.\n",
    "\n",
    "stop_tokens_voyage = [token for token in tokens_voyage if token in stopword_list]\n",
    "stop_tokens_jacob = [token for token in tokens_jacob if token in stopword_list]\n",
    "\n",
    "# create frequency distributions of the top tokens in each text. then print out\n",
    "# the ten most common tokens. since we previously filtered, these lists will contain\n",
    "# stopwords only.\n",
    "\n",
    "stop_freq_voyage = nltk.FreqDist(stop_tokens_voyage)\n",
    "stop_freq_jacob = nltk.FreqDist(stop_tokens_jacob)\n",
    "\n",
    "\n",
    "tokens_voyage = [token for token in tokens_voyage if token not in stopword_list]\n",
    "tokens_jacob = [token for token in tokens_jacob if token not in stopword_list]\n",
    "\n",
    "freq_voyage = nltk.FreqDist(tokens_voyage)\n",
    "freq_jacob = nltk.FreqDist(tokens_jacob)\n",
    "\n",
    "\n",
    "print('=====')\n",
    "print('Comparison of Texts with Stopwords Excluded')\n",
    "print('The Voyage Out')\n",
    "print(freq_voyage.most_common(10))\n",
    "print(\"Jacob's Room\")\n",
    "print(freq_jacob.most_common(10))\n",
    "\n",
    "print('=====')\n",
    "print('Comparison of Stopwords in the texts')\n",
    "print('The Voyage Out')\n",
    "print(stop_freq_voyage.most_common(15))\n",
    "print(\"Jacob's Room\")\n",
    "print(stop_freq_jacob.most_common(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, many of the same words appear in both lists. Note that Jacob's Room has a male name in the top ten most common words, while The Voyage Out does not. This makes sense - Jacob's Room is at least nominally about a character named Jacob. But this character is presented in an elliptical way, primarily through the descriptions and recollections of other characters, which is to some degree reflected in the fact that 'her' just barely edges out 'his' in the text. You could use this information in conjunction with other metrics about the text to make larger arguments about the representation of character and gender. We could combine this sort of analysis with one that takes into account punctuation use to get a better sense of an author's particular style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: look at this last paragarph to clean up the analysis - not sure it really works. Maybe it's that it gives you a more nuanced representation of character because you would assum that Jacob is the most important? not really about punctuation though. Or maybt it's that they don't actually look all that different? Maybe pull in dalloway, which is very different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
