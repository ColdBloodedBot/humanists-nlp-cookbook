{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords and Filtering\n",
    "\n",
    "In natural language processing, we frequently work with a list of stopwords, those words that occur so often in any given text in a language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words here are probably unsurprising: prepositions, pronouns, and similar words that show up frequently across English-language corpora. In most cases you will want to supplement such a list with your own words to create a stopwords list that makes sense for your corpus. If you work with early modern texts, for example, you probably have a whole range of different words and phrases particular to your period that you would want to keep in mind. \n",
    "\n",
    "It's common in NLP to use lists like these to create filter your text. Take take a piece of the first chunk of Jacob's room:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CHAPTER',\n",
       " 'ONE',\n",
       " \"''\",\n",
       " 'So',\n",
       " 'of',\n",
       " 'course',\n",
       " ',',\n",
       " \"''\",\n",
       " 'wrote',\n",
       " 'Betty',\n",
       " 'Flanders',\n",
       " ',',\n",
       " 'pressing',\n",
       " 'her',\n",
       " 'heels',\n",
       " 'rather',\n",
       " 'deeper',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sand',\n",
       " ',',\n",
       " '``',\n",
       " 'there',\n",
       " 'was',\n",
       " 'nothing',\n",
       " 'for',\n",
       " 'it',\n",
       " 'but',\n",
       " 'to',\n",
       " 'leave',\n",
       " '.',\n",
       " \"''\",\n",
       " 'Slowly',\n",
       " 'welling',\n",
       " 'from',\n",
       " 'the',\n",
       " 'point',\n",
       " 'of',\n",
       " 'her',\n",
       " 'gold',\n",
       " 'nib',\n",
       " ',',\n",
       " 'pale',\n",
       " 'blue',\n",
       " 'ink',\n",
       " 'dissolved',\n",
       " 'the',\n",
       " 'full',\n",
       " 'stop',\n",
       " ';']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_chunks(text, num_chunks):\n",
    "    text_length = len(text)\n",
    "    text_chunks = []\n",
    "    number_of_chunks = num_chunks\n",
    "    for i in range(number_of_chunks):\n",
    "        chunk_size = text_length/number_of_chunks\n",
    "        chunk_start = math.floor(chunk_size * i)\n",
    "        chunk_end = math.floor(chunk_size * (i +1))\n",
    "        text_chunks.append(text[chunk_start:chunk_end])\n",
    "    return text_chunks\n",
    "\n",
    "filename = 'corpus/1922_jacobs_room.txt'\n",
    "with open(filename, 'r') as fin:\n",
    "    raw_text = fin.read()\n",
    "\n",
    "chunked_text = get_chunks(raw_text, 100)\n",
    "tokenized_text = [nltk.word_tokenize(chunk) for chunk in chunked_text]\n",
    "jacob_counts = [nltk.FreqDist(tokenized_chunk)['Jacob'] for tokenized_chunk in tokenized_text]\n",
    "tokenized_text[0][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the stopwords list to filter out common words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHAPTER', 'ONE', \"''\", 'So', 'course', ',', \"''\", 'wrote', 'Betty', 'Flanders', ',', 'pressing', 'heels', 'rather', 'deeper', 'sand', ',', '``', 'nothing', 'leave', '.', \"''\", 'Slowly', 'welling', 'point', 'gold', 'nib', ',', 'pale', 'blue', 'ink', 'dissolved', 'full', 'stop', ';']\n"
     ]
    }
   ],
   "source": [
    "filtered_chunk = [token for token in tokenized_text[0][:50] if token not in nltk.corpus.stopwords.words('english')]\n",
    "print(filtered_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the words that are gone now: 'of', 'her', 'in', 'the'. Filtering words like this often gives us a sense of those words that are more likely to be meaningful for the particular text. Here, we get character names, as well as a sense of vocabulary that might be thought of as more particular to Woolf. Note how these words are particular to the content of her text - we get a sense of what she is writing about and how she is describing it. You could imagine comparing this list of vocabulary to another author and finding it to be quite different!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, even though we filtered out common words, we still have quite a lot of unwanted characters - punctuation. These grammatical markings are often filtered out in much the same way, using a different part of nltk. There are a couple different ways to do this. The simplest draws upon Python's built-in string class, which has a list of punctuation in it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\", '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "\n",
    "# create an initial stopword list by loading in the one from nltk\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# add punctuation to that list\n",
    "\n",
    "stopword_list.extend(string.punctuation)\n",
    "print(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHAPTER', 'ONE', \"''\", 'So', 'course', \"''\", 'wrote', 'Betty', 'Flanders', 'pressing', 'heels', 'rather', 'deeper', 'sand', '``', 'nothing', 'leave', \"''\", 'Slowly', 'welling', 'point', 'gold', 'nib', 'pale', 'blue', 'ink', 'dissolved', 'full', 'stop']\n"
     ]
    }
   ],
   "source": [
    "filtered_chunk = [token for token in tokenized_text[0][:50] if token not in stopword_list]\n",
    "print(filtered_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gets us closer - we pulled out a few punctuation marks. But notice that some still made it through. So we'll want to further extend our stopwords list based on the things that got left out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CHAPTER', 'ONE', 'So', 'course', 'wrote', 'Betty', 'Flanders', 'pressing', 'heels', 'rather', 'deeper', 'sand', 'nothing', 'leave', 'Slowly', 'welling', 'point', 'gold', 'nib', 'pale', 'blue', 'ink', 'dissolved', 'full', 'stop']\n"
     ]
    }
   ],
   "source": [
    "stopword_list.extend([\"''\", \"``\"])\n",
    "filtered_chunk = [token for token in tokenized_text[0][:50] if token not in stopword_list]\n",
    "print(filtered_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks more like it. We've successfully massaged out all the punctuation in this initial chunk, though further situations like this might come up later on. The lesson here is that textual data is _messy_, and even the most sophisticated natural language processing setups require a good deal of massaging and careful modification in light of particular situations.  Each text presents in own set of problems, and only through familiarity with your objects of study will you know what exactly needs to be accounted for. In short, there is no substitute for knowing your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are good reasons why you might actually want to leave those stopwords in. If these words are statistically the most common in English-language texts, then they must serve as meaningful points of comparison among many different texts. Put another way, any given two authors might take different objects of study in their texts. Filtering out common words can give a better sense of their particular interests. But the common words that they share might give a good sense of the particulars of their own literary style, the way in which they write. Let's use this principle to compare Jacob's Room with The Voyage Out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 6678), ('and', 4325), ('of', 3699), ('to', 3616), ('a', 2879), ('in', 2172), ('was', 2104), ('she', 1902), ('her', 1901), ('that', 1577)]\n",
      "[('the', 3560), ('and', 1673), ('of', 1321), ('a', 1061), ('to', 1007), ('in', 965), ('was', 678), ('her', 579), ('his', 521), ('with', 477)]\n"
     ]
    }
   ],
   "source": [
    "# store the names of both filenames\n",
    "\n",
    "fn_one = 'corpus/1915_the_voyage_out.txt'\n",
    "fn_two = 'corpus/1922_jacobs_room.txt'\n",
    "\n",
    "# read in the text of the_voyage_out and assign it a variable text_voyage\n",
    "\n",
    "with open(fn_one, 'r') as fin:\n",
    "    text_voyage = fin.read()\n",
    "\n",
    "# read in the text of jacobs_room and assign it a variable text_jacob\n",
    "    \n",
    "with open(fn_two, 'r') as fin:\n",
    "    text_jacob = fin.read()\n",
    "\n",
    "# read in the English-language stopwords list provided by nltk and store it in \n",
    "# a variable stopword_list\n",
    "\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "# use nltk's built-in tokenizer to get a list of the tokens in each text. store\n",
    "# these in variables.\n",
    "\n",
    "tokens_voyage = nltk.word_tokenize(text_voyage)\n",
    "tokens_jacob = nltk.word_tokenize(text_jacob)\n",
    "\n",
    "\n",
    "# look at the tokens we've got and filter out stopwords by comparing tokens against\n",
    "# the stored list of stopwords.\n",
    "\n",
    "tokens_voyage = [token for token in tokens_voyage if token in stopword_list]\n",
    "tokens_jacob = [token for token in tokens_jacob if token in stopword_list]\n",
    "\n",
    "# create frequency distributions of the top tokens in each text. then print out\n",
    "# the ten most common tokens. since we previously filtered, these lists will contain\n",
    "# stopwords only.\n",
    "\n",
    "\n",
    "\n",
    "freq_voyage = nltk.FreqDist(tokens_voyage)\n",
    "freq_jacob = nltk.FreqDist(tokens_jacob)\n",
    "\n",
    "print(freq_voyage.most_common(10))\n",
    "print(freq_jacob.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, many of the same words appear in both lists. Note that Jacob's Room has a male pronoun in the top ten most common words, while The Voyage Out does not. This makes sense - Jacob's Room is at least nominally about a character named Jacob. But this character is presented in an elliptical way, primarily through the descriptions and recollections of other characters, which is to some degree reflected in the fact that 'her' just barely edges out 'his' in the text. You could use this information in conjunction with other metrics about the text to make larger arguments about the representation of character and gender. We could combine this sort of analysis with one that takes into account punctuation use to get a better sense of an author's particular style."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
